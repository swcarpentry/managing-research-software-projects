<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>Managing Research Software Projects</title>
    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/sky.css">
    <link rel="stylesheet" href="lib/css/zenburn.css">
    <link rel="stylesheet" href="custom.css">
    <script>
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
    </script>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">

<!-- @ title page -->
<section style="text-align: center;">
  <h2>Managing Research Software Projects</h2>
  <p>&nbsp;</p>
  <h4><a href="http://software-carpentry.org">Software Carpentry Foundation</a></h4>
  <p>
    This is an early draft:
    <br/>
    please leave feedback in <a href="http://github.com/swcarpentry/managing-research-software-projects">the GitHub repository</a>.
  </p>
  <p>&nbsp;</p>
  <div>
    <div class="left">
      <h4>October 2016</h4>
    </div>
    <div class="right">
      <img src="img/cc-by.svg" alt="CC-BY" />
    </div>
  </div>
</section>

<!-- @ overview -->
<section data-markdown>
**Roadmap**

*   Key features of research software projects
*   What "done" looks like
*   Basics of project organization, usability
*   Social considerations
*   Critical fixtures: automated testing, version control
*   Making it all happen
</section>

<!-- @ scope of this talk -->
<section data-markdown>
**What Kinds of Projects?**

*   3x3: three people for three months
*   Contributors are frequently time-slicing other projects
*   "Everybody makes coffee"
</section>

<!-- @ what makes research projects special -->
<section data-markdown>
**Key Features of Research Software Projects**

*   Developers have extensive domain knowledge, but are largely self-taught programmers
*   Don't know all the right answers, but do know some
*   Requirements may be either:
    *   Discovered as we go along (exploring)
    *   Relatively stable (engineering)
*   Problem is *subtle* as well as *complicated*
*   Getting funding, credit can be difficult
</section>

<!-- @ goals -->
<section data-markdown>
**What "Done" Looks Like**

*   Software can be used by people other than original authors
    *   Reproducibility meaningless without this
*   Reasonably confident that results are correct
    *   As good as physical experiment
*   Small changes and extensions are easy
    *   Automated tests make change safe - [Feathers](https://www.amazon.com/Working-Effectively-Legacy-Michael-Feathers/dp/0131177052/)
*   Fast enough to be useful
</section>

<!-- @ Noble's Rules -->
<section data-markdown>
**[Noble's Rules](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000424)**

![Project organization](img/noble.png)
</section>
<section data-markdown>
**[Noble's Rules](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000424)**

How to organize small research projects:

*   Put each project in its own directory <!-- which is named after the project -->
*   Put text documents in `doc/`
*   Put raw data and metadata in `data/`
*   Put files generated during analysis in `results/` <!-- preliminary cleanup is part of the analysis; document, treat raw data as read-only -->
*   Put project source code in `src/`
*   Put external scripts, or compiled programs in `bin/`
*   Name all files to reflect their content or function

<!--
Adapt as necessary:
precise organization isn't really important,
but the underlying principles are.
 -->
</section>

<!-- @ Taschuk's Rules -->
<section data-markdown>
**[Taschuk's Rules](http://oicr-gsi.github.io/robust-paper/)**

How to make research software more robust:

*   Have a descriptive README (synopsis, dependencies)  <!-- Have a README that explains in a few lines what the software does and what its dependencies are -->
*   Provide a descriptive usage statement               <!-- Print usage information when launching from the command line that explains the software's features -->
*   Give the software a meaningful version number
*   Make older versions available
*   Reuse software (within reason)
</section>
<section data-markdown>
**[Taschuk's Rules](http://oicr-gsi.github.io/robust-paper/)** (cont.)

*   Do not require root or other special privileges
*   Eliminate hard-coded paths
*   Enable command-line configuration              <!-- Allow configuration of all useful parameters from the command line -->
*   Include a small data set to test installation  <!-- Include a small test set that can be run to ensure the software is actually working -->
*   Produce identical results when given identical inputs

From "works on my laptop" to "runs on your cluster".
</section>

<!-- @ Social considerations -->

<section data-markdown>
**Social considerations**

*   [Steinmacher](http://www.igor.pro.br/publica/papers/GSD_CSCW2014.pdf)'s analysis of barriers to contribution
    *   How easy is it to get set up?
    *   How friendly was reception of first contribution?
*   So add the following to Noble's Rules:
    *   `LICENSE`: terms of re-use
        *   **Use a standard license** (preferably MIT)
    *   `CITATION`: how to cite the software
        *   Get a DOI for the software (see [Zenodo](https://zenodo.org/))
    *   `CONTRIBUTING`: how to make contributions
    *   `CONDUCT`: project's social rules
</section>
<section data-markdown>
**Social considerations (cont.)**

* Simultaneously selfless and selfish
    * makes science easier to evaluate
    * makes life easier on your colleagues
    * makes it more likely that others will use (and **contribute to!**) your software
    * ensures relevancy of your work when funding runs out or maintainer moves on
    * more likely to impact traditional academic metrics (i.e. citations)
</section>

<section data-markdown>
**A Note on Code Review**

*   Academics review each other's papers; what about code?
*   Re: code review, [Petre](http://arxiv.org/abs/1407.5648) found:
    *   Requires domain knowledge to be useful
    *   No point doing it at submission time
*   But domain experts are scarce
*   No incentives to review someone else's thesis project code
*   At present, only sustainable within team projects
</section>

<!-- Automated testing -->
<section data-markdown>
**Automated testing**

*   Already have correctness tests in place (or should!)
*   Invest time to automate these tests
*   Common kinds of tests
    *   Smoke tests (sanity checks)                       <!-- literally, make sure the code doesn't blow up when invoked -->
    *   Unit tests (developer-facing)                     <!-- testing at scale of individual functions/methods -->
    *   Functional tests (user-facing)                    <!-- testing at scale of entire reusable components (scripts, classes, libraries) -->
    *   Regression tests (bug-facing)                     <!-- written when a bug is fixed -->
*   Execute frequently to ensure reliability of software  <!-- setting up for discussion of continuous integration -->
</section>

<!-- @ Version control -->
<section data-markdown>
**Does this look familiar?**

![Filenames](img/phd052810s.gif)
</section>
<section data-markdown>
**Discussion**

*   What are your best and worst stories of project organization?
    *   code and/or manuscript
    *   personal and collaborative
*   Consider on your own, then discuss with your neighbor:
    *   What worked well?
    *   What didn't work well?
</section>
<section data-markdown>
**Version control**

*   Provenance (reproducibility)
*   Transparency (credibility, trust)
*   Collaboration (work in parallel, peer review)
*   Hosting (distribution, bug/issue tracking)
*   Facilitates worry-free tinkering
*   Professional obligation! <!-- I would argue that it's a... -->

See [Ram, 2013](https://dx.doi.org/10.1186%2F1751-0473-8-7) and
[Blischak *et al.*, 2016](https://dx.doi.org/10.1371%2Fjournal.pcbi.1004668)
for an overview.
</section>

<!-- @ overview of agile vs. sturdy -->
<section data-markdown>
**From What to How**

*   This is the ideal: how do we get there?
*   "Traditional" software development is a planning-intensive engineering discipline
*   Didn't really have a name until "agile" came along in the 1990s
*   Now refer to the engineering approach as "sturdy"
*   Differences between the two are much smaller in practice than in theory
</section>
<section data-markdown>
**Agile vs. Sturdy**

*   Agile: rapid iteration
    *   informal / underdeveloped / changing requirements
    *   frequent (daily) short progress updates
    *   works well for small teams
*   Sturdy: "measure twice, cut once"
    *   formal / mature requirements
    *   more upfront planning and estimation
    *   scheduling enforced by manager(s)
    *   can scale to very large projects
*   We're going to focus on common themes
</section>

<section data-markdown>
**Pair Programming**

*   Real-time code review
*   Knowledge transfer ("we all make coffee")
*   Discourages Facebook and Twitter
    *   At least initially
*   Driver and navigator switch roles every hour
    *   With a short break to stay fresh
*   [Empirical studies](https://www.amazon.com/Making-Software-Really-Works-Believe/dp/0596808321/)
    confirm effectiveness
</section>

<section data-markdown>
**Test-Driven Development (TDD)**

*   Approach
    *   Write the test
    *   *Then* write the code
    *   Then clean up and commit
    *   "Red-green-refactor"
*   Writing tests first helps with design
    *   And ensures tests actually get written
*   [Empirical studies](https://www.amazon.com/Making-Software-Really-Works-Believe/dp/0596808321/)
    don't confirm effectiveness...
    *   ...but people who use it swear by it
</section>

<section data-markdown>
**Exercise**

*   Polygon overlay is one of the basic operations in geographic information systems.
*   Simplified version is rectangle overlay.
    *   Each rectangle is represented as [x0, y0, x1, y1].

![Rectangle Overlay](img/overlay.png)

*   Given two valid rectangles as input,
    `overlay` return the rectangle that is their overlap.
</section>

<section data-markdown>
**Exercise**

*   What are the three most interesting tests to write for this function?
*   Describe in terms of input 1, input 2, expected output, and why it's important.
    *   Assume input rectangles are well-formed (i.e., nobody's trying to pass in a character string).
*   Come up with answers on your own, then compare with your neighbor and select the best three.
</section>

<section data-markdown>
**Continuous Integration**

*   Tests are already automated, right...RIGHT?!
*   Re-run automated tests *before* merging changes
*   Ensures baseline is always in runnable state
*   CI as a service
    *   free for open-source projects
    *   pay for private projects
*   Run CI internally
    *   free open-source solutions
    *   bring your own hardware (& configuration)
</section>

<section data-markdown>
**Ticketing Systems**

*   A common fixture on code hosting services
*   Use them to track:
    *   What's broken
    *   What needs to be added
    *   Who's working on what
*   Great place to foster open discussion
</section>

<section data-markdown>
**Compromises**

<!--
*   Strict adherance isn't practical for most research projects
    * limited time, resources
-->
*   "Technical debt"
    * dissonance between conceptual model and model reflected in code  <!-- Strict definition -->
    * informed, deliberate suspension of best practice <!-- in the interest of Getting Things Doneâ„¢ --> <!-- More general usage -->
*   Necessary to get project off the ground
*   Complicates sustained development
    * debt must be paid down
</section>

<section data-markdown>
**Compromises (cont.)**

*   Iterative development
    *   Build a quick prototype  <!-- Best in a language like Python or R -->
    *   Test accuracy            <!-- Also much nicer in Python or R -->
    *   If performance is unsatisfactory:
        *   profile performance empirically
        *   optimize code surgically
    *   Clean up code
*   Compromise on best practices if you must
    * but not on version control
    * and not on automated testing
</section>

<section data-markdown>
**[Stupidity-Driven Development](http://ivory.idyll.org/blog/2014-research-coding.html)**

*   Write **lots** of tests for scientific core of the code
*   Start with many fewer tests for other project components
    *   command-line interface
    *   file I/O
    *   other "boring" and "unimportant" code
*   When bugs are encountered:
    *   fix the observed bug
    *   write new regression tests specific to that bug
    *   get on with more important things
</section>

<section data-markdown>
**[Stupidity-Driven Development](http://ivory.idyll.org/blog/2014-research-coding.html)**

*   Avoids wasting time writing tests for bugs that:
    *   never appear, or appear rarely
    *   don't affect correctness of results
*   Order of priorities
    *   correctness
    *   performance
    *   user experience
    *   ...
    *   beautiful code
</section>

<section data-markdown>
**Finally, A Note on Overtime**

*   [Robinson](http://www.igda.org/?page=crunchsixlessons):
    working overtime when you're late only makes you later
*   Every all nighter reduces cognitive function 25%
    *   So after two all nighters,
        you would not legally be considered competent to care for yourself
*   Optimal work cycle is:
    *   45-50 minutes of work + 10 minutes of activity
    *   8 hours/day
    *   5 days/week
</section>

      </div>
    </div>
    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.js"></script>
    <script>
      // More info https://github.com/hakimel/reveal.js#configuration
      // More info https://github.com/hakimel/reveal.js#dependencies
      Reveal.initialize({
        history: true,
        center: false,
        slideNumber: true,
        dependencies: [
          { src: 'plugin/markdown/marked.js' },
          { src: 'plugin/markdown/markdown.js' },
          { src: 'plugin/notes/notes.js', async: true },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
        ]
      });
    </script>
  </body>
</html>
